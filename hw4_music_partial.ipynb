{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Neural Networks for Music Classification\n",
    "\n",
    "In addition to the concepts in the MNIST neural network demo, in this lab, you will learn to:\n",
    "* Load a file from a URL\n",
    "* Extract simple features from audio samples for machine learning tasks such as speech recognition and classification\n",
    "* Build a simple neural network for music classification using these features\n",
    "* Record the loss and accuracy history in the training process\n",
    "* Optimize the learning rate of the neural network\n",
    "\n",
    "To illustrate the basic concepts, we will look at a relatively simple music classification problem.  Given a sample of music, we want to determine which instrument (e.g. trumpet, violin, piano) is playing.  This dataset was generously supplied by [Prof. Juan Bello](http://steinhardt.nyu.edu/faculty/Juan_Pablo_Bello) at NYU Stenihardt  and his former PhD student Eric Humphrey (now at Spotify).  They have a complete website dedicated to deep learning methods in music informatics:\n",
    "\n",
    "http://marl.smusic.nyu.edu/wordpress/projects/feature-learning-deep-architectures/deep-learning-python-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Loading the Keras package\n",
    "\n",
    "Before beginning you need to make sure you have installed tensorflow and keras.  Full instructions are available:\n",
    "*  [Tensorflow installation page](https://www.tensorflow.org/install/)\n",
    "*  [Keras installation page](https://keras.io/#installation)\n",
    "\n",
    "For the most part, you should be able to install both with the commands:\n",
    "     \n",
    "    pip3 install --upgrade tensorflow\n",
    "    pip3 install --upgrade keras\n",
    "    \n",
    "After you have installed the packages, we can begin by loading keras and the other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-681c49d5ba75>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-681c49d5ba75>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip3 install --upgrade tensorflow\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Feature Extraction with Librosa\n",
    "\n",
    "The key to audio classification is to extract the correct features. In addition to `keras`, we will need the `librosa` package.  The `librosa` package in python has a rich set of methods extracting the features of audio samples commonly used in machine learning tasks such as speech recognition and sound classification. \n",
    "\n",
    "Installation instructions and complete documentation for the package are given on the [librosa main page](https://librosa.github.io/librosa/).  On most systems, you should be able to simply use:\n",
    "\n",
    "    pip install -u librosa\n",
    "    \n",
    "For Unix, you may need to load some additional packages:\n",
    "\n",
    "    sudo apt-get install build-essential\n",
    "    sudo apt-get install libxext-dev python-qt4 qt4-dev-tools\n",
    "    pip install librosa\n",
    "    \n",
    "After you have installed the package, try to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use a set of music samples from the website:\n",
    "\n",
    "http://theremin.music.uiowa.edu\n",
    "\n",
    "This website has a great set of samples for audio processing.  Look on the web for how to use the `requests.get` and `file.write` commands to load the file at the URL provided into your working directory.\n",
    "\n",
    "You can play the audio sample by copying the file to your local machine and playing it on any media player.  If you listen to it you will hear a soprano saxaphone (with vibrato) playing four notes (C, C#, D, Eb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "fn = \"SopSax.Vib.pp.C6Eb6.aiff\"\n",
    "url = \"http://theremin.music.uiowa.edu/sound files/MIS/Woodwinds/sopranosaxophone/\"+fn\n",
    "\n",
    "# TODO:  Load the file from url and save it in a file under the name fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `librosa` command `librosa.load` to read the audio file with filename `fn` and get the samples `y` and sample rate `sr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# y, sr = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features from audio files is an entire subject on its own right.  A commonly used set of features are called the Mel Frequency Cepstral Coefficients (MFCCs).  These are derived from the so-called mel spectrogram which is something like a regular spectrogram, but the power and frequency are represented in log scale, which more naturally aligns with human perceptual processing.  You can run the code below to display the mel spectrogram from the audio sample.\n",
    "\n",
    "You can easily see the four notes played in the audio track.  You also see the 'harmonics' of each notes, which are other tones at integer multiples of the fundamental frequency of each note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(S),\n",
    "                         y_axis='mel', fmax=8000, x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data\n",
    "\n",
    "Using the MFCC features described above, Eric Humphrey and Juan Bellow have created a complete data set that can used for instrument classification.  Essentially, they collected a number of data files from the website above.  For each audio file, the segmented the track into notes and then extracted 120 MFCCs for each note.  The goal is to recognize the instrument from the 120 MFCCs.  The process of feature extraction is quite involved.  So, we will just use their processed data provided at:\n",
    "\n",
    "https://github.com/marl/dl4mir-tutorial/blob/master/README.md\n",
    "\n",
    "Note the password.  Load the four files into some directory, say  `instrument_dataset`.  Then, load them with the commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'instrument_dataset/'\n",
    "Xtr = np.load(data_dir+'uiowa_train_data.npy')\n",
    "ytr = np.load(data_dir+'uiowa_train_labels.npy')\n",
    "Xts = np.load(data_dir+'uiowa_test_data.npy')\n",
    "yts = np.load(data_dir+'uiowa_test_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data files:\n",
    "* What are the number of training and test samples?\n",
    "* What is the number of features for each sample?\n",
    "* How many classes (i.e. instruments) are there per class?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, you must scale the training and test data, `Xtr` and `Xts`.  Compute the mean and std deviation of each feature in `Xtr` and create a new training data set, `Xtr_scale`, by subtracting the mean and dividing by the std deviation.  Also compute a scaled test data set, `Xts_scale` using the mean and std deviation learned from the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Scale the training and test matrices\n",
    "# Xtr_scale = ...\n",
    "# Xts_scale = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building a Neural Network Classifier\n",
    "\n",
    "Following the example in MNIST neural network demo posted on CCLE on May 24th (there is both a pdf file and a python notebook file; you can save the python notebook file and open it in Jupyter or similiar tool), clear the keras session.  Then, create a neural network `model` with:\n",
    "* `nh=256` hidden units\n",
    "* `sigmoid` activation\n",
    "* select the input and output shapes correctly\n",
    "* print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear session (removes existing models)\n",
    "import keras.backend as K  # From demo (so you can use K.xxx instead of typing keras.backend.xxx)\n",
    "K.clear_session()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:  Print the model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an optimizer and compile the model.  Select the appropriate loss function and metrics.  For the optimizer, use the Adam optimizer with a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# opt = ...\n",
    "# model.compile(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model for 10 epochs using the scaled data for both the training and validation.  Use the `validation_data` option to pass the test data.  Use a batch size of 100.  Your final accuracy should be >99%.  To record the training history, use\n",
    "\n",
    "     hist = model.fit(...)\n",
    "     \n",
    "This will return a data structure, `hist` with the metrics per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# hist = model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structure `hist` should contain the training and validation accuracy as a function of the epoch:\n",
    "*  `hist.history['acc']`:  Training accuracy per epoch\n",
    "*  `hist.history['val_acc']`:  Test/validation accuracy per epoch\n",
    "\n",
    "Plot the training and test validation as a function of the epoch.  Label your graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function, stored in `hist.history['loss']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Learning Rate\n",
    "\n",
    "One challenge in training neural networks is the selection of the learning rate.  Rerun the above code, trying four learning rates as shown in the vector `rates`.  For each learning rate:\n",
    "* clear the session\n",
    "* construct the network\n",
    "* select the optimizer.  Use the Adam optimizer with the appropriate learrning rate.\n",
    "* train the model\n",
    "* save the accuracy and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rates = [0.01,0.001,0.0001]\n",
    "batch_size = 100\n",
    "loss_hist = []\n",
    "val_acc_hist = []\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function vs. the epoch number for all three learning rates on one graph.  In order that you can see the difference, it may be useful to plot them using `semilogy` so that the loss is in log scale. You should see that the lower learning rates are more stable, but converge slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
